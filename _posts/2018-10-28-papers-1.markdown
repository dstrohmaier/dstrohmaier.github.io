---
layout: post
title:  "CS Papers #1"
date:   2018-10-28 13:51:13 +0100
categories: reading
---

I intend to keep a log of the papers I'm reading in the course of my MPhil studies in CS. To limit the amount of time needed, I will only offer a very concise summary (1-3 sentences) and a similarly brief evaluation based on my reading and the discussion my courses. Feel free to challenge both summary and evaluation by dropping me an email!


McCallum A. & Nigam K. (1998): A Comparison of Event Models for Naive Bayes Text Classification
Summary: The common naive Bayes text classifier actually comes in two version, one of which considers only word absence and presence (multi-variate Bernoulli model), the other the word counts (multinomial model). The latter performs better, at least with a reasonable large vocabulary.
Evaluation: This classic paper helps a lot to get the distinction between the two classifiers clear. The data on the perfomance is strong, still I cannot help but wonder whether tweaking things there might reduce the differences between the two classifers.

David D. Lewis (1998): Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval
Summary: A very short introduction to naive Bayes, sticking to the basic math and intuitions.
Evaluation: Interestingly, this paper gives the multinomial model a rather short treatment and instead pays attention to a Poisson version. Lacking independent empirical testing it does not have as much force as the McCallum and Nigam paper.

Rogati M. & Yang Y. (2002): High-Perfoming Feature Selection for Text Classification
Summary: For text classification we have to select features, typically a vocabulary of words we consider. This paper compares the performance of various methods and suggests chi-square as a particularly good approach. 
Evaluation: I find it hard to get excited about the feature selection for straightforward ML methods such as naive Bayes, SVM and KNN. Evenn less so when it turns out the selection doesn't matter much for the best performing method in this paper: SVMs.

Gabrilovich E. & Markovitch S. (2006): Overcoming the Brittleness Bottleneck using Wikipedia: Enhacing Text Categorization with Encyclopieda Knowledge
Summary: To overcome the brittleness resulting from bag of word representations of documents, the authors enrich them by comparing the documents to Wikipedia articles and adding their concepts to the representation.
Evaluation: The way the authors present their results bug me. It is some basic feature augmentation by finding associated Wikipeida articles, but they try to sell it as introducting world knowledge. It does that in a rudimentary way at best, which is fair enough since the results are still good, but don't oversell it!