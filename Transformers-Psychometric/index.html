<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Selected Papers on Whether Transformers Converge with Psychometric Data &raquo; David Strohmaier</title>
  <meta name="description" content="In this list, I collect papers bearing on the question, whether transformermodels converge with human cognition as captured by psychometric data(eye-tracking...">

  <link rel="stylesheet" type="text/css" href="/assets/css/main.css">
  <link rel="canonical" href="https://dstrohmaier.com/Transformers-Psychometric/">
  <link rel="alternate" type="application/rss+xml" title="David Strohmaier" href="https://dstrohmaier.com/feed.xml">

  <!-- Icon stuff -->
  <link rel="shortcut icon" href="https://dstrohmaier.com/assets/images/favicon.ico">

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>


  <body>
      <div class="container">
          <div class="column left"></div>
          <div class="column middle">
              <div class="header">
                  <div class="nav">
    <ul>
        <li><a href="/">Home</a></li>
        <li><a href="/posts">Blog</a></li>
        <li><a href="/publications">Publications</a></li>
        <li><a href="/cv">CV</a></li>
        <li><a href="/lists">Reading Lists</a></li>
    </ul>
</div>

              </div >
              <article>

  <header>
    <h1>Selected Papers on Whether Transformers Converge with Psychometric Data</h1>
    <p class="byline">
      <span>By David Strohmaier</span>
      <time>&middot; Dec 31, 2022</time>
    </p>
  </header>

  <p>In this list, I collect papers bearing on the question, whether transformer
models converge with human cognition as captured by psychometric data
(eye-tracking, fMRI, etc.).</p>

<h2 id="core-papers">Core Papers</h2>

<ul>
  <li>Caucheteux, C., Gramfort, A., &amp; King, J.-R. (2022). Deep language algorithms predict semantic comprehension from brain activity. Scientific Reports, 12(1), Article 1. <a href="https://doi.org/10.1038/s41598-022-20460-9">https://doi.org/10.1038/s41598-022-20460-9</a></li>
  <li>Caucheteux, C., &amp; King, J.-R. (2022). Brains and algorithms partially converge in natural language processing. Communications Biology, 5(1), Article 1. <a href="https://doi.org/10.1038/s42003-022-03036-1">https://doi.org/10.1038/s42003-022-03036-1</a></li>
  <li>Merkx, D., &amp; Frank, S. L. (2021). Human Sentence Processing: Recurrence or Attention? Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, 12–22. <a href="https://doi.org/10.18653/v1/2021.cmcl-1.2">https://doi.org/10.18653/v1/2021.cmcl-1.2</a></li>
  <li>Michaelov, J. A., Bardolph, M. D., Coulson, S., &amp; Bergen, B. K. (2021). Different kinds of cognitive plausibility: Why are transformers better than RNNs at predicting N400 amplitude? ArXiv:2107.09648 [Cs]. <a href="http://arxiv.org/abs/2107.09648">http://arxiv.org/abs/2107.09648</a></li>
  <li>Oh, B.-D., Clark, C., &amp; Schuler, W. (2021). Surprisal Estimators for Human Reading Times Need Character Models. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 3746–3757. <a href="https://doi.org/10.18653/v1/2021.acl-long.290">https://doi.org/10.18653/v1/2021.acl-long.290</a></li>
  <li>Schrimpf, M., Blank, I. A., Tuckute, G., Kauf, C., Hosseini, E. A., Kanwisher, N., Tenenbaum, J. B., &amp; Fedorenko, E. (2021). The neural architecture of language: Integrative modeling converges on predictive processing. Proceedings of the National Academy of Sciences, 118(45), e2105646118. <a href="https://doi.org/10.1073/pnas.2105646118">https://doi.org/10.1073/pnas.2105646118</a></li>
  <li>Wilcox, E. G., Gauthier, J., Hu, J., Qian, P., &amp; Levy, R. (2020). On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior (arXiv:2006.01912). arXiv. <a href="https://doi.org/10.48550/arXiv.2006.01912">https://doi.org/10.48550/arXiv.2006.01912</a></li>
</ul>

<p>See also <a href="/transformers-and-the-brain/">my post on this issue</a>.</p>


  
  
  

  <table width="100%" border="0">
    <tb>
      <tr>
        
        
      </tr>
      <tr>
        
        
      </tr>
    </tb>
  </table>

</article>
 <footer>

  <table width="100%" border="0">
      <tr>
          <td>
              Find me on:
              <a href="https://sigmoid.social/@davidstrohmaier">
                  <em>Mastodon</em>
              </a>  &middot;
            <a href="https://www.linkedin.com/in/david-strohmaier-688893109">
                <em>Linkedin</em>
            </a> &middot;
            <a href="mailto:david.strohmaier@cl.cam.ac.uk">
                <em>david.strohmaier@cl.cam.ac.uk</em>
            </a>  &middot;
            <a href="/feed.xml">
                <em>RSS</em>
            </a>
          </td>
      </tr>
  </table>

</footer>

          </div>
          <div class="column right"><div class="nav">
    <ul>
        <li><a href="/">Home</a></li>
        <li><a href="/posts">Blog</a></li>
        <li><a href="/publications">Publications</a></li>
        <li><a href="/cv">CV</a></li>
        <li><a href="/lists">Reading Lists</a></li>
    </ul>
</div>
</div>
      </div>
  </body>

</html>
